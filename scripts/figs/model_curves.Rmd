---
title: "R Notebook"
output: html_notebook
---

```{r, message = F}
# !diagnostics off
library(dplyr)
library(ggplot2)
library(tidyr)
require(cowplot)
library(ggsignif)
library(Biostrings)
library(stringr)
library(pracma)

options(stringsAsFactors = F)

data <- read.table("../../processed_data/endo_tss/lb/rLP5_Endo2_lb_expression_formatted_std.txt",
                    header = T, fill  = T)
```

```{r, warning = F, message = F}
#This code uses the pwms from bTSSFinder (which come from literature) to find the best -10 site
A = c(0.0097,	1.0000,	0.3363,	0.5335,	0.4963,	0.0781)
C = c(0.0618,	0.0000,	0.1190,	0.1094,	0.2299,	0.0268)
G = c(0.1042,	0.0000,	0.0856,	0.1317,	0.1399,	0.0000)
T = c(0.8244,	0.0000,	0.4591,	0.2254,	0.1339,	0.8951)
pwm_s70 <- data.frame(A,C,G,T)
pwm_s70 <- t(pwm_s70)

data$minus10_matches <- sapply(as.character(data$variant),
                                      function(x) matchPWM(pwm_s70, str_sub(x,-150,-1)))

data$minus10_scores <- sapply(data$minus10_matches,
                                     function(x) PWMscoreStartingAt(pwm_s70, 
                                                                    subject(x),
                                                                    start(x)))
# set empty lists to NA
data$minus10_scores[sapply(data$minus10_scores, length) == 0] <- NA
data$minus10_max_score <- sapply(data$minus10_scores, max)
data$minus10_max_entry <- sapply(data$minus10_scores, which.max)
data$minus10_scores <- sapply(data$minus10_matches,
                                     function(x) PWMscoreStartingAt(pwm_s70, 
                                                                    subject(x),
                                                                    start(x)))

for (i in seq(1,length(data$minus10_matches))) {
    if(is.na(data$minus10_max_score[i])){
       data$minus10_max_score[i] <- 0
       data$minus10_start[i] <- -1
       data$minus10[i] <- "NNNNNN"
    }
    else{
      start_pos <- data$minus10_matches[i][[1]]@ranges@start[data$minus10_max_entry[[i]]]
      data$minus10_start[i] <- start_pos
      data$minus10[i] <- str_sub(data$variant[i], start_pos, start_pos+5)
    }
}

data <- select(data, -minus10_max_entry)
```


```{r warning=FALSE, message = F}
#This code uses the pwms from bTSSFinder (which come from literature) to find the best -35 site
A = c(0.0000,	0.0784,	0.0362,	0.4894,	0.3605,	0.4208)
C = c(0.1109,	0.0656,	0.0747,	0.2851,	0.3605,	0.0769)
G = c(0.1267,	0.0181,	0.6192,	0.1041,	0.0000,	0.2225)
T = c(0.7624,	0.8379,	0.2700,	0.1214,	0.2790,	0.2798)
pwm_s70_35 <- data.frame(A,C,G,T)
pwm_s70_35 <- t(pwm_s70_35)

data$minus35_matches <- sapply(as.character(data$variant),
                               function(x) matchPWM(pwm_s70_35, str_sub(x,-150,-1))) #Normally (x,-150,-1)
data$minus35_scores <- sapply(data$minus35_matches,
                              function(x) PWMscoreStartingAt(pwm_s70_35, 
                                                             subject(x),
                                                             start(x)))
data$minus35_scores[sapply(data$minus35_scores, length) == 0] <- NA
data$minus35_max_score <- sapply(data$minus35_scores, max)
data$minus35_max_entry <- sapply(data$minus35_scores,
                                 which.max)
data$minus35_scores <- sapply(data$minus35_matches,
                              function(x) PWMscoreStartingAt(pwm_s70_35, 
                                                             subject(x),
                                                             start(x)))

for (i in seq(1,length(data$minus35_matches))) {
    if(is.na(data$minus35_max_score[i])){
       data$minus35_max_score[i] <- 0
       data$minus35_start[i] <- -1
       data$minus35[i] <- "NNNNNN"
    }
    else{
      start_pos <- data$minus35_matches[i][[1]]@ranges@start[data$minus35_max_entry[[i]]]
      data$minus35_start[i] <- start_pos
      data$minus35[i] <- str_sub(data$variant[i],start_pos,start_pos+5)
    }
}

data <- select(data, -minus35_max_entry)
```

```{r}
# paired scoring of PWMs with variable spacing
paired_pwm_scoring <- function(seq, minus10, minus35, spacer_length) {
    # grab scores at every position
    scores_minus10 <- matchPWM(minus10, seq, min.score = 0, with.score = TRUE)
    scores_minus35 <- matchPWM(minus35, seq, min.score = 0, with.score = TRUE)
    score_length <- length(mcols(scores_minus10)$score)
    # subtract six to account for length of minus 35 and spacer
    paired_scores <- mcols(scores_minus10)$score[1:(score_length - spacer_length - 6)] +
        mcols(scores_minus35)$score[(spacer_length + 6 + 1) : score_length]
    return(max(paired_scores))
}

data$pwm_paired_max_16bp <- sapply(as.character(data$variant),
                                   function(x) paired_pwm_scoring(x, 
                                                                  pwm_s70, 
                                                                  pwm_s70_35,
                                                                  16))
data$pwm_paired_max_17bp <- sapply(as.character(data$variant),
                                   function(x) paired_pwm_scoring(x, 
                                                                  pwm_s70, 
                                                                  pwm_s70_35,
                                                                  17))
data$pwm_paired_max_18bp <- sapply(as.character(data$variant),
                                   function(x) paired_pwm_scoring(x, 
                                                                  pwm_s70, 
                                                                  pwm_s70_35,
                                                                  18))

# create column for best score among three spacer lengths and which length
data <- data %>% 
    rowwise() %>% 
    mutate(pwm_paired_max = max(pwm_paired_max_16bp, pwm_paired_max_17bp, pwm_paired_max_18bp))

write.table(select(data, -minus35_scores, -minus35_matches, -minus10_matches, -minus10_scores),
            '../../processed_data/endo_tss/lb/tss_expression_pwm_info.txt', 
            row.names = F, quote = F, sep = '\t')
```


```{r}
# simple linear model based on max -35 and -10 score
minus35and10_linear_fit <- lm(expn_med_fitted_scaled ~ minus35_max_score + minus10_max_score, data)
summary(minus35and10_linear_fit)
```


```{r}
data$predicted <- predict(minus35and10_linear_fit, data)
r.squared_minus35and10 <- summary(lm(expn_med_fitted_scaled ~ predicted, data))$r.squared
ggplot(data, aes(predicted, expn_med_fitted_scaled)) + geom_point() + 
    geom_smooth(method = 'lm') + 
    annotate('text', x = 0.5, y= 100, parse = T, label = paste('R^2==', signif(r.squared_minus35and10, 3))) +
    scale_x_log10() + scale_y_log10() + annotation_logticks() +
    labs(title = 'Linear regression model using max -35 and max -10',
         y = 'observed')
```

```{r}
linear_accuracy <- rmserr(data$predicted, data$expn_med_fitted_scaled)
```

```{r}
no_zeroes <- filter(data, expn_med_fitted_scaled != 0) %>% 
    mutate(logged = log(expn_med_fitted_scaled))
log_fit <- lm(logged ~ minus35_max_score + minus10_max_score, no_zeroes)
summary(log_fit)
```

```{r}
data$predicted_log <- predict(log_fit, data)
no_zeroes$predicted_log <- predict(log_fit, no_zeroes)
r.squared_minus35and10_log <- summary(lm(log(expn_med_fitted_scaled) ~ predicted_log, no_zeroes))$r.squared
ggplot(data, aes(exp(predicted_log), expn_med_fitted_scaled)) + geom_point() + 
    geom_smooth(method = 'lm') + 
    annotate('text', x = 0.65, y = 10, parse = T, label = paste('R^2==', signif(r.squared_minus35and10_log, 3))) +
    scale_y_log10() + annotation_logticks() +
    labs(title = 'Linear regression model using max -35 and max -10 (log-transformed)',
         x = 'predicted', y = 'observed')
```

```{r}
log_accuracy <- rmserr(no_zeroes$predicted_log, no_zeroes$logged)
log_accuracy
```


Let's calculate GC content for each sequence

```{r}
calc_gc <- function(seq) {
    seq_length <- nchar(seq)
    noG <- gsub('G', '', seq)
    noGC <- gsub('C', '', noG)
    numGC <- seq_length - nchar(noGC)
    gc_content <- numGC / seq_length
    return(gc_content)
}

data$gc_content <- calc_gc(data$variant)

ggplot(data, aes(gc_content, expn_med_fitted_scaled)) + geom_point() +
    scale_y_log10() +
    labs(x = 'GC Content', y = 'expression')
```

```{r}
summary(lm(expn_med_fitted_scaled ~ gc_content, data))
```

```{r}
summary(lm(expn_med_fitted_scaled ~ minus35_max_score + minus10_max_score + gc_content + pwm_paired_max, 
           filter(data, expn_med_fitted_scaled > 1)))
```

Create training sets for 75% of the genome and test on remaining 25%. Combine TSS,
scramble and peak libraries and "floor" the data by setting all values less than 1 equal to 1.
This should encourage the learning models to pay less attention to the noise.

```
python define_genome_splits.py 0.75 4639675 ../../processed_data/combined/tss_scramble_peak_expression_model_format.txt ../../processed_data/combined/tss_scramble_peak_expression_model_format.txt --floor
```

Run neural network hyperparameter tuning on all the data, on raw one-hot encoded DNA
```
python dragonn_hyperparameter_tuning_regression.py ../../processed_data/combined/tss_scramble_peak_expression_model_format_floored_train_genome_split.txt ../../processed_data/combined/tss_scramble_peak_expression_model_format_floored_test_genome_split.txt 150 4 5 100 0.2 100 ../../processed_data/combined/20190522_tss_scramble_peak_floored_hyperparam_tuned > ../../processed_data/combined/20190522_tss_scramble_peak_floored_hyperparam_tuned.log
```

MAE - mean absolute error. Average of the absolute difference between predicted and observed.
Linear score which means all the individual differences are weighted equally in the average.
RMSE - root mean squared error. Sample standard deviation of differences between predicted and observed. 
Penalizes higher differences more than MAE.
MAPE - mean absolute percentage error/deviation. Expresses accuracy as a percentage. Average absolute percent
error, works best if there are no extremes (and no zeros)
NMSE - normalized mean squared error. Normalizing MSE facilitates comparisons between datasets or models with
different scales.
RSTD - relative standard deviation. Tells you whether the "regular" standard deviation is a small or
large quantity when compared to the mean. 

```{r}
library(pracma)
nn <- read.table('../../processed_data/combined/20190522_tss_scramble_peak_floored_hyperparam_tuned_predictions.txt',
                 col.names = c('sequence', 'predicted', 'observed'))
nn_accuracy <- rmserr(nn$predicted, nn$observed)
```

```{r}
r.squared_nn <- summary(lm(observed ~ predicted, nn))$r.squared
ggplot(nn, aes(predicted, observed)) + geom_point() +
    geom_smooth(method = 'lm') + 
    annotate('text', x = 30, y = 1, parse = T, label = paste('R^2==', signif(r.squared_nn, 3))) +
    scale_x_log10() + scale_y_log10() + annotation_logticks() +
    labs(title = 'Neural network (one-hot encoding)')
```

Generate k-mer counts for libraries.

```{r}
library(kmer)
# read in pre-defined splits
train <- read.table('../../processed_data/combined/tss_scramble_peak_expression_model_format_floored_train_genome_split.txt',
                            col.names = c('sequence', 'expn_med_fitted_scaled'))

seq_list <- strsplit(train$sequence, '')
train_counts <- as.data.frame(kcount(seq_list, k = 6))

bind_cols(train, train_counts) %>% 
    write.table('../../processed_data/combined/tss_scramble_peak_expression_model_format_floored_train_genome_split_kmer_counts.txt', 
                row.names = F, quote = F, sep = '\t', col.names = F)

# read in pre-defined splits
test <- read.table('../../processed_data/combined/tss_scramble_peak_expression_model_format_floored_test_genome_split.txt',
                            col.names = c('sequence', 'expn_med_fitted_scaled'))

seq_list <- strsplit(test$sequence, '')
test_counts <- as.data.frame(kcount(seq_list, k = 6))

bind_cols(test, test_counts) %>% 
    write.table('../../processed_data/combined/tss_scramble_peak_expression_model_format_floored_test_genome_split_kmer_counts.txt', 
                row.names = F, quote = F, sep = '\t', col.names = F)
```

Run random forest regression on k-mer counts. One-hot encoding isn't suitable for random forest.

```
python random_forest_regression.py ../../processed_data/combined/tss_scramble_peak_expression_model_format_floored_train_genome_split_kmer_counts.txt ../../processed_data/combined/tss_scramble_peak_expression_model_format_floored_test_genome_split_kmer_counts.txt ../../processed_data/combined/rfr_tss_scramble_peak_floored_predictions.txt
```

```{r}
rfr <- read.table('../../processed_data/combined/rfr_tss_scramble_peak_floored_predictions.txt',
                              col.names = c('predicted', 'observed'))
rfr_accuracy <- rmserr(rfr$predicted, rfr$observed)
rfr_accuracy
```

```{r}
r.squared_rfr <- summary(lm(observed ~ predicted, rfr))$r.squared
ggplot(rfr, aes(predicted, observed)) + geom_point() +
    geom_smooth(method = 'lm') + 
    annotate('text', x = 30, y = 1, parse = T, label = paste('R^2==', signif(r.squared_rfr, 3))) +
    scale_x_log10() + scale_y_log10() + annotation_logticks() +
    labs(title = 'Random forest regression (k-mer counts)')
```


Let's compare this to raw one-hot encoded DNA sequence, the same feature set the neural
network uses. Random forests aren't suited to this type of data because it increases the dimensionality.
For example the nucleotide A is encoded 0001 so the random forest sees this as four separate
features (0-0-0-1).

```
python random_forest_regression.py ../../processed_data/combined/tss_scramble_peak_expression_model_format_floored_train_genome_split.txt ../../processed_data/combined/tss_scramble_peak_expression_model_format_floored_test_genome_split.txt ../../processed_data/combined/rfr_onehot_tss_scramble_peak_floored_predictions.txt --onehot --seq_length 150
```

```{r}
rfr_onehot <- read.table('../../processed_data/combined/rfr_onehot_tss_scramble_peak_floored_predictions.txt',
                                     col.names = c('predicted', 'observed'))
rfr_onehot_accuracy <- rmserr(rfr_onehot$predicted, rfr_onehot$observed)
rfr_onehot_accuracy
```

```{r}
r.squared_rfr_onehot <- summary(lm(observed ~ predicted, rfr_onehot))$r.squared
ggplot(rfr_onehot, aes(predicted, observed)) + geom_point() +
    geom_smooth(method = 'lm') + 
    annotate('text', x = 30, y = 1, parse = T, label = paste('R^2==', signif(r.squared_rfr_onehot, 3))) +
    scale_x_log10() + scale_y_log10() + annotation_logticks() +
    labs(title = 'Random forest regression (one-hot DNA))')
```


```{r}
mlp <- read.table('../../processed_data/combined/mlp_tss_scramble_peak_predictions.txt',
                  col.names = c('predicted', 'observed'))

r.squared_mlp <- summary(lm(observed ~ predicted, mlp))$r.squared
ggplot(mlp, aes(predicted, observed)) + geom_point() +
    geom_smooth(method = 'lm') + 
    annotate('text', x = 100, y = 0.5, parse = T, label = paste('R^2==', signif(r.squared_mlp, 3))) +
    scale_x_log10() + scale_y_log10() + annotation_logticks() +
    labs(title = 'Multilayer perceptron (6-mers)')
```

```{r}
mlp_accuracy <- rmserr(mlp$predicted, mlp$observed)
mlp_accuracy
```


```{r}
mlp_3to6 <- read.table('../../processed_data/combined/20190608_mlp_tss_3to6mer_scramble_peak_predictions.txt',
                       col.names = c('predicted', 'observed'))
r.squared_mlp_3to6 <- summary(lm(observed ~ predicted, mlp_3to6))$r.squared
ggplot(mlp_3to6, aes(predicted, observed)) + geom_point() +
    geom_smooth(method = 'lm') + 
    annotate('text', x = 100, y = 0.5, parse = T, label = paste('R^2==', signif(r.squared_mlp_3to6, 3))) +
    scale_x_log10() + scale_y_log10() + annotation_logticks() +
    labs(title = 'Multilayer perceptron (3 to 6-mers)')
```

```{r}
mlp_3to6_accuracy <- rmserr(mlp_3to6$predicted, mlp_3to6$observed)
mlp_3to6_accuracy
```

Next, I generated k-mer counts for the training set, from length 3 to 6. I wanted to filter
out k-mers that weren't strongly correlated to expression. I created k-mer counts for
random genomic sequences, equal to the size of the training set. For each k-mer, I calculated the
correlation between the k-mer and expression for training and random. I only keep a k-mer
if it does better than "random". Using this process I eliminate ~500 k-mers for a total of 4916.
I trained a simple multiple linear regression on the training set and predicted the test set.

```{r}
kmer_predictions <- read.table('../../processed_data/combined/kmer_linear_predictions.txt',
                               col.names = c('predicted', 'observed'))
r.squared_kmer <- summary(lm(observed ~ predicted, kmer_predictions))$r.squared
ggplot(kmer_predictions, aes(predicted, observed)) + geom_point() +
    geom_smooth(method = 'lm') +
    scale_x_log10() + scale_y_log10() + annotation_logticks(sides = 'bl') +
    annotate('text', x = 0.001, y = 100, parse = T, label = paste('R^2==', signif(r.squared_kmer, 3))) +
    labs(title = '3 to 6-mers multiple linear regression')
```

```{r}
kmer_accuracy <- rmserr(kmer_predictions$observed, kmer_predictions$predicted)
kmer_accuracy
```

```{r}
kmer_pls_predictions <- read.table('../../processed_data/combined/kmer_pls_predictions.txt',
                               col.names = c('predicted', 'observed'))
r.squared_kmer_pls <- summary(lm(observed ~ predicted, kmer_pls_predictions))$r.squared
ggplot(kmer_pls_predictions, aes(predicted, observed)) + geom_point() +
    geom_smooth(method = 'lm') +
    scale_x_log10() + scale_y_log10() + annotation_logticks(sides = 'bl') +
    annotate('text', x = 0.0001, y = 100, parse = T, label = paste('R^2==', signif(r.squared_kmer_pls, 3))) +
    labs(title = '3 to 6-mers partial least squares regression')
```

```{r}
pls_accuracy <- rmserr(kmer_pls_predictions$predicted, kmer_pls_predictions$observed)
pls_accuracy
```

```{r}
# gkMSVM for TSS only
gkmsvm <- read.table('../../processed_data/endo_tss/lb/model_files/gkmsvm_10mer_8ungapped_kernel_test_results',
                     col.names = c('name', 'predicted'))
tss_test <- read.table('../../processed_data/endo_tss/lb/model_files/tss_expression_model_format_test_genome_split.txt',
                   header = F, sep = '\t', col.names = c('sequence', 'expression'))
gkmsvm$observed <- tss_test$expression
```

```{r}
library(PRROC)
pr_curve <- pr.curve(scores.class0 = filter(gkmsvm, observed >= 1)$predicted,
                     scores.class1 = filter(gkmsvm, observed < 1)$predicted,
                     curve = T)
plot(pr_curve)
```

```{r}
accuracy <- bind_rows(data.frame(nn_accuracy, method = 'neural network'),
                      data.frame(rfr_accuracy, method = 'random forest regression (6-mer)'),
                      data.frame(linear_accuracy, method = 'linear -35 and -10'),
                      data.frame(linear_accuracy, method = 'linear -35 and -10 (log)'),
                      data.frame(rfr_onehot_accuracy, method = 'random forest regression (one-hot)'),
                      data.frame(mlp_accuracy, method = 'multi-layer perceptron (6-mer)'),
                      data.frame(mlp_3to6_accuracy, method='multi-layer perceptron(3 to 6-mer)'),
                      data.frame(kmer_accuracy, method='linear regression (3 to 6-mer)'),
                      data.frame(pls_accuracy, method='PLS regression (3 to 6-mer)'))

accuracy$r.squared <- c(r.squared_nn, r.squared_rfr, r.squared_minus35and10, r.squared_minus35and10_log,
                        r.squared_rfr_onehot, r.squared_mlp, r.squared_mlp_3to6, r.squared_kmer,
                        r.squared_kmer_pls)
```

```{r}
gg1 <- ggplot(accuracy, aes(method, rmse)) + geom_bar(stat = 'identity') +
    labs(x = '', y = 'RMSE') +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
gg2 <- ggplot(accuracy, aes(method, r.squared)) + geom_bar(stat = 'identity') +
    labs(x = '', y = 'R-squared') +
    scale_y_continuous(limits = c(0, 1)) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

plot_grid(gg1, gg2, nrow = 1)
```

```{r}
nn_curve <- pr.curve(scores.class0 = filter(nn, observed >= 1)$predicted,
                     scores.class1 = filter(nn, observed < 1)$predicted, curve = T)
plot(nn_curve)
```

```{r}
kmer_curve <- pr.curve(scores.class0 = filter(kmer_predictions, observed >= 1)$predicted,
                     scores.class1 = filter(kmer_predictions, observed < 1)$predicted, curve = T)
plot(kmer_curve)
```


Let's filter the data to observed values > 1 and calculate the statistics.

```{r}
n_tss <- nrow(data)
n_test <- nrow(mlp)

# data <- left_join(data, select(no_zeroes, name, predicted_log), by = 'name')
predictions <- bind_rows(bind_cols(select(data, observed=expn_med_fitted_scaled, predicted), 
                                   method=rep('linear -35 and -10', n_tss)),
                         bind_cols(select(data, observed=expn_med_fitted_scaled, predicted = predicted_log),
                                   method=rep('linear -35 and -10 (log)', n_tss)),
                         bind_cols(mlp, method=rep('multi-layer perceptron (6-mer)', n_test)),
                         bind_cols(mlp_3to6, method=rep('multi-layer perceptron (3 to 6-mer', n_test)),
                         bind_cols(select(nn, -sequence), method=rep('neural network', n_test)),
                         bind_cols(rfr, method=rep('random forest regression (one-hot)', n_test)),
                         bind_cols(rfr_onehot, method=rep('random forest regression (6-mer)', n_test))) %>% 
    ungroup()

rmse_func <- function(df) {
    stats <- rmserr(df$predicted, df$observed)
    return(stats)
}

prediction_stats <- predictions %>% 
    filter(observed >= 1) %>% 
    group_by(method) %>% 
    do(value = rmse_func(.)) %>% 
    unnest() %>% 
    ungroup()

prediction_stats$variable <- c('mae', 'mse', 'rmse', 'mape', 'nmse', 'rstd')
prediction_stats <- prediction_stats %>% 
    spread(variable, value)

prediction_stats <- predictions %>% 
    group_by(method) %>% 
    do(r.squared = summary(lm(observed ~ predicted, .))$r.squared) %>% 
    ungroup() %>% 
    unnest() %>% 
    left_join(prediction_stats, ., by = 'method')
```

```{r}
gg1 <- ggplot(prediction_stats, aes(method, rmse)) + geom_bar(stat = 'identity') +
    labs(x = '', y = 'RMSE') +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
gg2 <- ggplot(prediction_stats, aes(method, r.squared)) + geom_bar(stat = 'identity') +
    labs(x = '', y = 'R-squared') +
    scale_y_continuous(limits = c(0, 1)) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

plot_grid(gg1, gg2, nrow = 1)
```





